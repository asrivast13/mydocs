{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Optional, Callable\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    AutoModelForCTC\n",
    ")\n",
    "from huggingsound.utils import get_chunks, get_waveforms, get_dataset_from_dict_list\n",
    "from huggingsound.token_set import TokenSet\n",
    "from huggingsound.normalizer import DefaultTextNormalizer\n",
    "from huggingsound.speech_recognition.decoder import Decoder, GreedyDecoder\n",
    "\n",
    "import collections\n",
    "import contextlib\n",
    "import wave\n",
    "import glob\n",
    "\n",
    "from traceback import print_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705676ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wave(path):\n",
    "    \"\"\"Reads a .wav file.\n",
    "    Takes the path, and returns (PCM audio data, sample rate).\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "        num_channels = wf.getnchannels()\n",
    "        assert num_channels == 1\n",
    "        sample_width = wf.getsampwidth()\n",
    "        assert sample_width == 2\n",
    "        sample_rate = wf.getframerate()\n",
    "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
    "        pcm_data = wf.readframes(wf.getnframes())\n",
    "        return pcm_data, sample_rate\n",
    "\n",
    "def write_wave(path, audio, sample_rate):\n",
    "    \"\"\"Writes a .wav file.\n",
    "    Takes path, PCM audio data, and sample rate.\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'wb')) as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(audio)\n",
    "\n",
    "class Frame(object):\n",
    "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
    "    def __init__(self, bytes, timestamp, duration, is_speech = True):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "        self.isSpeech = is_speech\n",
    "\n",
    "def __frame_generator_old(frame_duration_ms, audio, sample_rate, vad=None):\n",
    "    \"\"\"Generates audio frames from PCM audio data.\n",
    "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
    "    the sample rate.\n",
    "    Yields Frames of the requested duration.\n",
    "    \"\"\"\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "    offset = 0\n",
    "    timestamp = 0.0\n",
    "    duration = (float(n) / sample_rate) / 2.0\n",
    "    while offset + n < len(audio):\n",
    "        is_speech = vad.is_speech(audio[offset:offset + n], sample_rate) if vad is not None else False\n",
    "        yield Frame(audio[offset:offset + n], timestamp, duration, is_speech)\n",
    "        timestamp += duration\n",
    "        offset += n\n",
    "        \n",
    "class Utterance(object):\n",
    "    \"\"\"Represents an utterance of speech audio data.\"\"\"\n",
    "    def __init__(self, audio, timestamp, duration, bytes):\n",
    "        self.audio = audio\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "        self.bytes=bytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webrtcvad\n",
    "import numpy as np\n",
    "class AudioSegmenter(object):\n",
    "    \"\"\"Represents the WebRTC based Audio Segmentation tool\"\"\"\n",
    "    def __init__(self, \n",
    "                 vadAggressiveness=2, \n",
    "                 frameDurationMs=30, \n",
    "                 numFramesInWindow=100, \n",
    "                 samplingRate=16000,\n",
    "                 numBytesPerSample=2):\n",
    "        assert numBytesPerSample == 2  ## for now although the algo should work for byte encodings\n",
    "        self.vad = webrtcvad.Vad(vadAggressiveness)\n",
    "        self.sample_rate = samplingRate\n",
    "        self.frame_duration_ms = frameDurationMs\n",
    "        self.num_frames_in_window = numFramesInWindow\n",
    "        self.SCALE_FACTOR = 1./float(1 << ((8 * numBytesPerSample)-1))\n",
    "        self.num_bytes_per_sample = numBytesPerSample\n",
    "        self.minAudioFileDurationInSecs = 0\n",
    "\n",
    "    def set_min_file_size_for_segmentation(self, minAudioFileDurInSecsToSegment):\n",
    "        if(minAudioFileDurInSecsToSegment > 0):\n",
    "            self.minAudioFileDurationInSecs = minAudioFileDurInSecsToSegment\n",
    "\n",
    "    def frame_generator(self, audio):\n",
    "        \"\"\"Generates audio frames from PCM audio data.\n",
    "        Takes the desired frame duration in milliseconds, the PCM data, and\n",
    "        the sample rate.\n",
    "        Yields Frames of the requested duration.\n",
    "        \"\"\"\n",
    "        n = int(self.sample_rate * (self.frame_duration_ms / 1000.0) * 2)\n",
    "        offset = 0\n",
    "        timestamp = 0.0\n",
    "        duration = (float(n) / self.sample_rate) / 2.0\n",
    "        while offset + n < len(audio):\n",
    "            is_speech = self.vad.is_speech(audio[offset:offset + n], self.sample_rate)\n",
    "            yield Frame(audio[offset:offset + n], timestamp, duration, is_speech)\n",
    "            timestamp += duration\n",
    "            offset += n\n",
    "\n",
    "            \n",
    "    def vad_collector(self, \n",
    "                      frames, \n",
    "                      triggerToggleFactor=0.9, \n",
    "                      minSpeechInUtteranceInSecs=0.5, \n",
    "                      utteranceRunoffDuration=5, \n",
    "                      maxUtteranceDuration=30, \n",
    "                      minSilenceAtEnds=0.06, \n",
    "                      verbosity=0):\n",
    "        \n",
    "        \"\"\"Filters out non-voiced audio frames.\n",
    "        Given a webrtcvad.Vad and a source of audio frames, yields only\n",
    "        the voiced audio.\n",
    "        Uses a padded, sliding window algorithm over the audio frames.\n",
    "        When more than (triggerToggleFactor)X% (default X=90) of the frames in the window are voiced (as\n",
    "        reported by the VAD), the collector triggers and begins yielding\n",
    "        audio frames. Then the collector waits until X% of the frames in\n",
    "        the window are unvoiced to detrigger.\n",
    "        The window is padded at the front and back to provide a small\n",
    "        amount of silence or the beginnings/endings of speech around the\n",
    "        voiced frames.\n",
    "        Arguments:\n",
    "        sample_rate - The audio sample rate, in Hz.\n",
    "        frame_duration_ms - The frame duration in milliseconds.\n",
    "        padding_duration_ms - The amount to pad the window, in milliseconds.\n",
    "        vad - An instance of webrtcvad.Vad.\n",
    "        frames - a source of audio frames (sequence or generator).\n",
    "        Returns: A generator that yields PCM audio data.\n",
    "        \"\"\"\n",
    "        frame_duration_ms = self.frame_duration_ms\n",
    "        num_padding_frames = self.num_frames_in_window\n",
    "        sample_rate = self.sample_rate\n",
    "        min_silence_frames_at_end = int(minSilenceAtEnds * 1000 / frame_duration_ms)\n",
    "        if verbosity>2: \n",
    "            print('Min Silence Frames at End: %d' % min_silence_frames_at_end)\n",
    "        \n",
    "        # We use a deque for our sliding window/ring buffer.\n",
    "        ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "        # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
    "        # NOTTRIGGERED state.\n",
    "        triggered = False\n",
    "        start_time = -1\n",
    "        end_time = -1\n",
    "\n",
    "        ## smoothing on the frames\n",
    "        numFrames = len(frames)\n",
    "        for index, frame in enumerate(frames):\n",
    "            if(index > 0) and (index < (numFrames-1)):\n",
    "                if((frames[index].isSpeech != frames[index-1].isSpeech) \n",
    "                   and (frames[index-1].isSpeech == frames[index+1].isSpeech)):\n",
    "                    frames[index].isSpeech = frames[index-1].isSpeech\n",
    "\n",
    "        voiced_frames = []\n",
    "        segid = 1\n",
    "        num_silence_frames_at_end = 0\n",
    "        for frame in frames:\n",
    "            is_speech = frame.isSpeech\n",
    "\n",
    "            if verbosity > 3:\n",
    "                sys.stdout.write('1' if is_speech else '0')\n",
    "                sys.stdout.write(' %.2f\\n' % frame.timestamp)\n",
    "            if not triggered:\n",
    "                ring_buffer.append((frame, is_speech))\n",
    "                num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "                # If we're NOTTRIGGERED and more than 90% of the frames in\n",
    "                # the ring buffer are voiced frames, then enter the\n",
    "                # TRIGGERED state.\n",
    "                if num_voiced > triggerToggleFactor * ring_buffer.maxlen:\n",
    "                    triggered = True\n",
    "                    if verbosity > 3:\n",
    "                        sys.stdout.write('+(%s)\\n' % (ring_buffer[0][0].timestamp,))\n",
    "                    start_time = ring_buffer[0][0].timestamp\n",
    "                    # We want to yield all the audio we see from now until\n",
    "                    # we are NOTTRIGGERED, but we have to start with the\n",
    "                    # audio that's already in the ring buffer.\n",
    "                    for f, s in ring_buffer:\n",
    "                        voiced_frames.append(f)\n",
    "                    ring_buffer.clear()\n",
    "                    index = len(voiced_frames)-1\n",
    "                    while(index>=0):\n",
    "                        if(voiced_frames[index].isSpeech):\n",
    "                            break\n",
    "                        else:\n",
    "                            num_silence_frames_at_end += 1\n",
    "\n",
    "            else:\n",
    "                # We're in the TRIGGERED state, so collect the audio data\n",
    "                # and add it to the ring buffer.\n",
    "                voiced_frames.append(frame)\n",
    "                ring_buffer.append((frame, is_speech))\n",
    "                num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "                if not is_speech:\n",
    "                    num_silence_frames_at_end += 1\n",
    "                numSpeechFramesInUtterance = sum([int(f.isSpeech) for f in voiced_frames])\n",
    "                end_time = frame.timestamp + frame.duration\n",
    "                # If more than 90% of the frames in the ring buffer are\n",
    "                # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
    "                # audio we've collected.\n",
    "                if((numSpeechFramesInUtterance * frame_duration_ms / 1000) < minSpeechInUtteranceInSecs):\n",
    "                    continue\n",
    "                    \n",
    "                if ((num_unvoiced > (triggerToggleFactor * ring_buffer.maxlen)) or \n",
    "                (((end_time - start_time) > utteranceRunoffDuration) and (num_silence_frames_at_end >= min_silence_frames_at_end)) or\n",
    "                   ((end_time - start_time) > maxUtteranceDuration)\n",
    "                   ):\n",
    "                    if verbosity > 3:\n",
    "                        sys.stdout.write('-(%s)\\n' % (frame.timestamp + frame.duration))\n",
    "                    end_time = frame.timestamp + frame.duration\n",
    "                    #start_time -= minSilenceAtEnds if start_time >= minSilenceAtEnds else 0.0\n",
    "                    triggered = False\n",
    "                    databytes = b''.join([f.bytes for f in voiced_frames])\n",
    "                    audiosamples = np.frombuffer(databytes, dtype=np.int16).astype(np.float32)\n",
    "                    audiosamples *= self.SCALE_FACTOR\n",
    "                    if verbosity > 1:\n",
    "                        print('Segment %d: start=%.2f end=%.2f bytes=%d duration=%.2f\\n' % (segid, start_time, end_time, len(databytes), len(audiosamples)/sample_rate))\n",
    "                    segid += 1\n",
    "                    #databytes = None\n",
    "                    yield(Utterance(audiosamples, start_time, (end_time - start_time), databytes))\n",
    "                    start_time = -1\n",
    "                    end_time = -1\n",
    "                    ring_buffer.clear()\n",
    "                    voiced_frames = []\n",
    "                    num_silence_frames_at_end = 0\n",
    "        if verbosity > 3:\n",
    "            if triggered:\n",
    "                sys.stdout.write('-(%s)\\n' % (frame.timestamp + frame.duration))\n",
    "            sys.stdout.write('\\n')\n",
    "        # If we have any leftover voiced audio when we run out of input,\n",
    "        # yield it.\n",
    "        end_time = frame.timestamp + frame.duration\n",
    "        if voiced_frames:\n",
    "            databytes = b''.join([f.bytes for f in voiced_frames])\n",
    "            audiosamples = np.frombuffer(databytes, dtype=np.int16).astype(np.float32)\n",
    "            audiosamples *= self.SCALE_FACTOR\n",
    "            if verbosity > 1:\n",
    "                print('Segment %d: start=%.2f end=%.2f bytes=%d duration=%.2f\\n' % (segid, start_time, end_time, len(databytes), len(audiosamples)/sample_rate))\n",
    "            #databytes = None\n",
    "            yield(Utterance(audiosamples, start_time, (end_time - start_time), databytes))\n",
    "\n",
    "    def process(self, audio, \n",
    "                triggerToggleFactor=0.9, \n",
    "                minSpeechInUtteranceInSecs=0.5, \n",
    "                utteranceRunoffDuration=5, \n",
    "                maxUtteranceDuration=30, \n",
    "                minSilenceAtEnds=0.06):\n",
    "        \n",
    "        frames = self.frame_generator(audio)\n",
    "        frames = list(frames)\n",
    "        \n",
    "        totalAudioDuration = len(audio) / (self.sample_rate * self.num_bytes_per_sample)\n",
    "        segmentsList = []\n",
    "        \n",
    "        if(totalAudioDuration < self.minAudioFileDurationInSecs):\n",
    "            databytes = b''.join([f.bytes for f in frames])\n",
    "            audiosamples = np.frombuffer(databytes, dtype=np.int16).astype(np.float32)\n",
    "            audiosamples *= self.SCALE_FACTOR\n",
    "            segid = 0\n",
    "            start_time = 0.0\n",
    "            end_time = totalAudioDuration\n",
    "            print('Segment from File %d: start=%.2f end=%.2f bytes=%d duration=%.2f\\n' % (segid, start_time, end_time, len(databytes), len(audiosamples)/sample_rate))\n",
    "            segment = Utterance(audiosamples, start_time, (end_time - start_time), None)\n",
    "            segmentsList.append(segment)\n",
    "        else:\n",
    "            totalSpeechDuration = 0.0\n",
    "            totalNumSegments = 0\n",
    "            maxDuration = 0.0\n",
    "            speech_segments = self.vad_collector(frames, triggerToggleFactor, minSpeechInUtteranceInSecs, utteranceRunoffDuration, maxUtteranceDuration, minSilenceAtEnds)\n",
    "            for segment in speech_segments:\n",
    "                if segment.duration > maxDuration:\n",
    "                    maxDuration = segment.duration\n",
    "                totalSpeechDuration += segment.duration\n",
    "                totalNumSegments += 1\n",
    "                segmentsList.append(segment)\n",
    "\n",
    "            print('Segmenter found total of %d segments with duration %.2fsecs from audio of duration %.2fsecs with max=%.2f -- compression factor: %.2f%%' \n",
    "                      % (totalNumSegments, totalSpeechDuration, totalAudioDuration, maxDuration, (100.0*totalSpeechDuration/totalAudioDuration)))\n",
    "        return segmentsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4532879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class SpeechRecognitionModel2():\n",
    "    \"\"\"\n",
    "    Speech Recognition Model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str\n",
    "        The path to the model or the model identifier from huggingface.co/models.\n",
    "    \n",
    "    device: Optional[str] = \"cpu\"\n",
    "        Device to use for inference/evaluation/training, default is \"cpu\". If you want to use a GPU for that, \n",
    "        you'll probably need to specify the device as \"cuda\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, device: Optional[str] = \"cpu\"):\n",
    "        \n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        \n",
    "        logger.info(\"Loading model...\")\n",
    "        self._load_model()\n",
    "\n",
    "    @property\n",
    "    def is_finetuned(self):\n",
    "        return self.processor is not None\n",
    "\n",
    "    def _load_model(self):\n",
    "\n",
    "        self.model = AutoModelForCTC.from_pretrained(self.model_path)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        try:\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(self.model_path)\n",
    "            self.token_set = TokenSet.from_processor(self.processor)\n",
    "        except Exception:\n",
    "            logger.warning(\"Not fine-tuned model! You'll need to fine-tune it before use this model for audio transcription\")\n",
    "            self.processor = None\n",
    "            self.token_set = None\n",
    "\n",
    "    def transcribeFiles(self, paths: list[str], batch_size: Optional[int] = 1, decoder: Optional[Decoder] = None) -> list[dict]:\n",
    "        \"\"\" \n",
    "        Transcribe audio files.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "            paths: list[str]\n",
    "                List of paths to audio files to transcribe\n",
    "\n",
    "            batch_size: Optional[int] = 1\n",
    "                Batch size to use for inference\n",
    "\n",
    "            decoder: Optional[Decoder] = None\n",
    "                Decoder to use for transcription. If you don't specify this, the engine will use the GreedyDecoder.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "            list[dict]:\n",
    "                A list of dictionaries containing the transcription for each audio file:\n",
    "\n",
    "                [{\n",
    "                    \"transcription\": str,\n",
    "                    \"start_timesteps\": list[int],\n",
    "                    \"end_timesteps\": list[int],\n",
    "                    \"probabilities\": list[float]\n",
    "                }, ...]\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.is_finetuned:\n",
    "            raise ValueError(\"Not fine-tuned model! Please, fine-tune the model first.\")\n",
    "        \n",
    "        if decoder is None:\n",
    "            decoder = GreedyDecoder(self.token_set)\n",
    "\n",
    "        sampling_rate = self.processor.feature_extractor.sampling_rate\n",
    "        result = []\n",
    "\n",
    "        for paths_batch in tqdm(list(get_chunks(paths, batch_size))):\n",
    "\n",
    "            waveforms = get_waveforms(paths_batch, sampling_rate)\n",
    "\n",
    "            inputs = self.processor(waveforms, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True, do_normalize=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(inputs.input_values.to(self.device), attention_mask=inputs.attention_mask.to(self.device)).logits\n",
    "\n",
    "            result += decoder(logits)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def transcribeAudio(self, utterances: list[Utterance], batch_size: Optional[int] = 1, decoder: Optional[Decoder] = None) -> list[dict]:\n",
    "        \"\"\" \n",
    "        Transcribe audio files.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "            paths: list[Utterance]\n",
    "                List of audio utterances to transcribe\n",
    "\n",
    "            batch_size: Optional[int] = 1\n",
    "                Batch size to use for inference\n",
    "\n",
    "            decoder: Optional[Decoder] = None\n",
    "                Decoder to use for transcription. If you don't specify this, the engine will use the GreedyDecoder.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "            list[dict]:\n",
    "                A list of dictionaries containing the transcription for each audio file:\n",
    "\n",
    "                [{\n",
    "                    \"transcription\": str,\n",
    "                    \"start_timesteps\": list[int],\n",
    "                    \"end_timesteps\": list[int],\n",
    "                    \"probabilities\": list[float]\n",
    "                }, ...]\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.is_finetuned:\n",
    "            raise ValueError(\"Not fine-tuned model! Please, fine-tune the model first.\")\n",
    "\n",
    "        if decoder is None:\n",
    "            decoder = GreedyDecoder(self.token_set)\n",
    "\n",
    "        sampling_rate = self.processor.feature_extractor.sampling_rate\n",
    "        result = []\n",
    "\n",
    "        for utts_batch in tqdm(list(get_chunks(utterances, batch_size))):\n",
    "\n",
    "            #waveforms = get_waveforms(paths_batch, sampling_rate)\n",
    "            waveforms = []\n",
    "            for utt in utts_batch:\n",
    "                waveforms.append(utt.audio)\n",
    "\n",
    "            inputs = self.processor(waveforms, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True, do_normalize=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(inputs.input_values.to(self.device), attention_mask=inputs.attention_mask.to(self.device)).logits\n",
    "\n",
    "            batchResults = decoder(logits)\n",
    "            for index, br in enumerate(batchResults):\n",
    "                if(len(br['transcription'].strip()) == 0):\n",
    "                    continue;\n",
    "                br['utterance_start']    = '%.2f' % utts_batch[index].timestamp\n",
    "                br['utterance_duration'] = '%.2f' % utts_batch[index].duration\n",
    "                br['tokens'] = self.convertToCTM(br)\n",
    "                result.append(br)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def convertToCTM(self, utterance: dict, extrapolate: Optional[bool] = False) -> dict:\n",
    "        transcript  = utterance[\"transcription\"]\n",
    "        uttstart    = float(utterance['utterance_start'])\n",
    "        duration    = float(utterance['utterance_duration'])\n",
    "        char_starts = utterance[\"start_timestamps\"]\n",
    "        char_ends   = utterance[\"end_timestamps\"]\n",
    "        tokens = []\n",
    "\n",
    "        if((char_starts is None) \n",
    "           or (char_ends is None) \n",
    "           or (len(char_starts) == 0) \n",
    "           or (len(char_ends) == 0) or \n",
    "           (len(transcript) != len(char_starts)) \n",
    "           or (len(char_starts) != len(char_ends)) \n",
    "           or (char_ends[-1] > (duration * 1000))):\n",
    "            extrapolate = True\n",
    "\n",
    "        if(extrapolate):\n",
    "            words = transcript.split()\n",
    "            if(len(words) == 0):\n",
    "                return tokens\n",
    "            numChars = len(transcript)\n",
    "            start = uttstart\n",
    "            start += 0.1\n",
    "            dur = 0.0\n",
    "            for word in words:\n",
    "                dur = duration * len(word) / numChars\n",
    "                #ctmline = \"%s 1 %.2f %.2f %s\" % (sessionId, start, dur, word)\n",
    "                ctm = {\"baseform\": word, \"start\": start, \"duration\": dur}\n",
    "                start += dur\n",
    "                tokens.append(ctm)\n",
    "        else:\n",
    "            word = \"\"\n",
    "            start = -1\n",
    "            dur = 0.0\n",
    "            for index, char in enumerate(transcript):\n",
    "                if(char == \" \"):\n",
    "                    if(len(word)>0):\n",
    "                        #ctmline = \"%s 1 %.2f %.2f %s\" % (sessionId, (uttstart + start/1000), dur/1000, word)\n",
    "                        wst = (uttstart + start/1000)\n",
    "                        wd  = dur/1000\n",
    "                        ctm = {\"baseform\": word, \"start\": wst, \"duration\": wd}\n",
    "                        tokens.append(ctm)\n",
    "                        word = \"\"\n",
    "                        start = -1\n",
    "                        dur = 0.0\n",
    "                else:\n",
    "                    if(len(word)==0):\n",
    "                        start = char_starts[index]\n",
    "                    word += char\n",
    "                    dur = char_ends[index] - start\n",
    "\n",
    "            if(len(word)>0):\n",
    "                #ctmline = \"%s 1 %.2f %.2f %s\" % (sessionId, (uttstart + start/1000), dur/1000, word)\n",
    "                wst = (uttstart + start/1000)\n",
    "                wd  = dur/1000\n",
    "                ctm = {\"baseform\": word, \"start\": wst, \"duration\": wd}\n",
    "                tokens.append(ctm)\n",
    "                word = \"\"\n",
    "                start = -1\n",
    "                dur = 0.0\n",
    "\n",
    "        return tokens\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf3806",
   "metadata": {},
   "source": [
    "from traceback import print_tb\n",
    "import torch\n",
    "import os.path\n",
    "import argparse\n",
    "#from huggingsound import SpeechRecognitionModel, KenshoLMDecoder\n",
    "\n",
    "device = \"cuda\" if (torch.cuda.is_available()) else \"cpu\"\n",
    "print('device = %s' % device)\n",
    "#device='cpu'\n",
    "#SttModelFolder = \"/Users/asrivast/Models/wav2vec2-large-xlsr-53-english/\"\n",
    "SttModelFolder = \"/home/asrivast/Models/wav2vec2-large-xlsr-53-spanish/\"\n",
    "model = SpeechRecognitionModel2(SttModelFolder, device=device)\n",
    "print(model.processor.feature_extractor.sampling_rate)\n",
    "useLM = True\n",
    "decoder = None\n",
    "if useLM == True:\n",
    "    from huggingsound import KenshoLMDecoder\n",
    "    LmModelFolder = SttModelFolder + \"/language_model/\"\n",
    "    lm_path = LmModelFolder + \"lm.binary\"\n",
    "    unigrams_path = LmModelFolder + \"unigrams.txt\"\n",
    "    decoder = KenshoLMDecoder(model.token_set, lm_path=lm_path, unigrams_path=unigrams_path, alpha=2, beta=1, beam_width=100)\n",
    "    print(\"Finished loading Language Model\")\n",
    "\n",
    "transcripts = model.transcribeAudio(segments, 1, decoder)\n",
    "#print(transcripts)\n",
    "\n",
    "for transcript in transcripts:\n",
    "    print('%s (%s-%s)\\n' % (transcript['transcription'], transcript['utterance_start'], transcript['utterance_duration']))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define input\n",
    "#audioFile = '/home/asrivast/d/Data/Speech/MagicData/Spanish_Conversational_Speech_Test_Corpus/WAV/A0001_S003_0_G0001_G0002.wav'\n",
    "\n",
    "InputFileOrFolder='/home/asrivast/d/Data/Speech/MagicData/Spanish_Conversational_Speech_Test_Corpus/WAV'\n",
    "SttModelFolder = \"/home/asrivast/Models/wav2vec2-large-xlsr-53-spanish/\"\n",
    "outFolder='/tmp'\n",
    "sampleRate = 16000\n",
    "useGPUifAvailable = True\n",
    "useLM = False\n",
    "\n",
    "##define constants\n",
    "aggressiveness = 3\n",
    "frameDurationMs = 30\n",
    "numFramesInWindow = 20\n",
    "\n",
    "## initialize segmenter\n",
    "print('Initializing Audio Segmenter with aggressiveness=%d frame length=%dmsec window size=%dframes and sample rate=%d\\n'\n",
    "     % (aggressiveness, frameDurationMs, numFramesInWindow, sampleRate))\n",
    "segmenter = AudioSegmenter(aggressiveness, frameDurationMs, numFramesInWindow, sampleRate, 2)\n",
    "\n",
    "## initialize STT model including LM if required\n",
    "device = \"cuda\" if (useGPUifAvailable and torch.cuda.is_available()) else \"cpu\"\n",
    "print('device = %s' % device)\n",
    "print('Loading speech recognition model from folder %s\\n' % SttModelFolder)\n",
    "model = None\n",
    "decoder = None\n",
    "\n",
    "try:\n",
    "    model = SpeechRecognitionModel2(SttModelFolder, device=device)\n",
    "    if useLM == True:\n",
    "        from huggingsound import ParlanceLMDecoder\n",
    "        LmModelFolder = SttModelFolder + \"/language_model/\"\n",
    "        lm_path = LmModelFolder + \"lm.binary\"\n",
    "        unigrams_path = LmModelFolder + \"unigrams.txt\"\n",
    "        # To use this decoder you'll need to install the Parlance's ctcdecode first (https://github.com/parlance/ctcdecode)\n",
    "        print('Starting to load LM file %s in ParlanceLMDecoder ...' % lm_path)\n",
    "        decoder = ParlanceLMDecoder(model.token_set, lm_path=lm_path, alpha=2, beta=1, beam_width=100)    \n",
    "        #decoder = KenshoLMDecoder(model.token_set, lm_path=lm_path, unigrams_path=unigrams_path, alpha=2, beta=1, beam_width=100)\n",
    "        print(\"Finished loading Language Model\")\n",
    "except Exception as e:\n",
    "    print('Could not load acoustic model. Failed with message: %s' % e)\n",
    "    sys.exit(-1)\n",
    "    \n",
    "audioFilesList = []\n",
    "if(os.path.isfile(InputFileOrFolder)):\n",
    "    audioFilesList.append(InputFileOrFolder)\n",
    "elif (os.path.isdir(InputFileOrFolder)):\n",
    "    audioFilesList = glob.glob(InputFileOrFolder+'/*.wav')\n",
    "else:\n",
    "    raise argparse.ArgumentTypeError('Invalid input: '+InputFileOrFolder)\n",
    "\n",
    "for audioFile in audioFilesList:\n",
    "    #read audio file into buffer\n",
    "    audio, sample_rate = read_wave(audioFile)\n",
    "    assert sample_rate == sampleRate\n",
    "\n",
    "    sessionId = os.path.basename(audioFile).split('.')[0]\n",
    "    print('Processing (%s %dHz %.2fsecs) in file %s' % (sessionId, sample_rate, (len(audio)/(sample_rate * 2)), audioFile))\n",
    "\n",
    "    ## Run segmenter with following parameters:\n",
    "    # threshold for speech/non-speech in buffer to trigger or not = 75%\n",
    "    # minimum speech length in utterance = 0.5 secs (below this the segmenter will not break)\n",
    "    # utterance runoff duration = 5 secs (above this the segmenter starts looking for X non-speech frames to break)\n",
    "    # max duration length = 15 secs (hard-limit ... might cause a break within a word)\n",
    "    # minimum number of non-speech frames needed at the end of the utterance to break (X) = 1 \n",
    "    segments = segmenter.process(audio, 0.75, 0.5, 5, 15, frameDurationMs/1000)\n",
    "    \n",
    "    # Now run the STT decoder on all the segments sequentially (could this be parallelized?)\n",
    "    transcripts = model.transcribeAudio(segments, 1, decoder)\n",
    "    \n",
    "    ## Write CTM lines into the output file\n",
    "    outFile = '%s/%s.ctm' % (outFolder, sessionId)\n",
    "    print('Writing %d transcripts into file %s' % (len(transcripts), outFile))\n",
    "    with open(outFile, 'w') as ofp:\n",
    "        for transcript in transcripts:\n",
    "            #print('\\t%s (%s-%s-%s)\\n' % (transcript['transcription'], sessionId, transcript['utterance_start'], transcript['utterance_duration']))\n",
    "            for token in transcript[\"tokens\"]:\n",
    "                ofp.write(\"%s \\t 1 \\t %.2f \\t %.2f \\t %s\\n\" % (sessionId, token[\"start\"], token[\"duration\"], token[\"baseform\"]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494f78a",
   "metadata": {},
   "source": [
    "## This block can save audio for each segment into files and run decoder on individual files to compare to the previous block \n",
    "from traceback import print_tb\n",
    "import torch\n",
    "import os.path\n",
    "import argparse\n",
    "from huggingsound import SpeechRecognitionModel, KenshoLMDecoder\n",
    "\n",
    "outFolder = '/tmp/'\n",
    "device = \"cuda\" if (torch.cuda.is_available()) else \"cpu\"\n",
    "SttModelFolder = \"/Users/asrivast/Models/wav2vec2-large-xlsr-53-english/\"\n",
    "model = SpeechRecognitionModel2(SttModelFolder, device=device)\n",
    "print(model.processor.feature_extractor.sampling_rate)\n",
    "useLM = False\n",
    "decoder = None\n",
    "if useLM == True:\n",
    "  LmModelFolder = SttModelFolder + \"/language_model/\"\n",
    "  lm_path = LmModelFolder + \"lm.binary\"\n",
    "  unigrams_path = LmModelFolder + \"unigrams.txt\"\n",
    "  decoder = KenshoLMDecoder(model.token_set, lm_path=lm_path, unigrams_path=unigrams_path, alpha=2, beta=1, beam_width=100)\n",
    "  print(\"Finished loading Language Model\")\n",
    "\n",
    "audioFilesList = []\n",
    "for i, segment in enumerate(segments):\n",
    "    path = outFolder\n",
    "    path = path + ('chunk-%002d.wav' % (i,)) \n",
    "    print('\\nWriting %s %.2f %.2f %d\\n' % (path, segment.timestamp, segment.duration, len(segment.audio)))\n",
    "    write_wave(path, segment.bytes, sample_rate)\n",
    "    audioFilesList.append(path)\n",
    "\n",
    "transcripts = model.transcribeFiles(audioFilesList, 1, decoder)\n",
    "#print(transcripts)\n",
    "#audioFilesList.clear()\n",
    "\n",
    "\n",
    "for transcript in transcripts:\n",
    "    print('%s\\n' % (transcript['transcription']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1478e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
